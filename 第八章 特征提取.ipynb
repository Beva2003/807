{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 第八章 特征提取\n",
    "\n",
    "本章的特征提取指的是特征变换。\n",
    "\n",
    "特征变换和前面的特征选择的目的都是将特征降维，但是特征选择是直接删掉一些特征，而特征提取是通过数学方法把高维特征映射为低维。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 一、基于类别可分性判据的特征提取\n",
    "\n",
    "这一节介绍的方法是通过最大化类内类间可分性判据$J$来对特征进行线性变换降维，也就是找到一个$\\mathbf{W}$，将样本$\\mathbf{x}$投影为$\\mathbf{y = \\mathbf{W}^T\\mathbf{x}}$，用投影后的$\\mathbf{y}$算出来的可分性判据$J$最大。\n",
    "\n",
    "书上的推导过程需要先学会标量对矩阵求导，我在[补充的数学知识.ipynb](./补充的数学知识.ipynb)中有介绍。\n",
    "\n",
    "经过一系列推导，最终的方法是：\n",
    "\n",
    "1. 求出矩阵$\\mathbf{S}_{\\text{w}}^{-1}\\mathbf{S}_{\\text{b}}$。\n",
    "\n",
    "关于$\\mathbf{S}_{\\text{w}}$和$\\mathbf{S}_{\\text{b}}$，书上在第四章和第七章中的表述不一致，我认为应该这里的公式应该采取第七章的说法，即\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{S}_{\\text{b}} &= \\sum_{i=1}^cP_i(\\mathbf{m}_i - \\mathbf{m})(\\mathbf{m}_i - \\mathbf{m})^T \\\\\n",
    "\\mathbf{S}_{\\text{w}} &= \\sum_{i=1}^cP_i\\frac{1}{n_i}\\sum_{k=1}^{n_i}(\\mathbf{x}_k^{(i)}-\\mathbf{m}_i)(\\mathbf{x}_k^{(i)}-\\mathbf{m}_i)^T\n",
    "\\end{align}$$\n",
    "\n",
    "其中$P_i$采用样本中类别的比例来进行估算，其实如果这样算的话公式里的$P_i\\frac{1}{n_i}$就变成了$\\frac{1}{n}$。\n",
    "\n",
    "2. 求出$\\mathbf{S}_{\\text{w}}^{-1}\\mathbf{S}_{\\text{b}}$的特征值和特征向量。\n",
    "3. 把特征值从大到小排序，选取前面最大的$d$个特征值的特征向量作为$\\mathbf{W}$。$d$是人为划定的，你想把样本降维成几维，$d$就是多大。\n",
    "\n",
    "计算特征值可以用`np.linalg.eig`方法来实现。`np.linalg.eig`返回值包含两部分，特征值和每个特征值对应的列向量（注意这里是列向量）。\n",
    "\n",
    "接下来是代码实现，还是注意和前几章同样的问题，numpy中向量是行向量，代码中实现的矩阵是公式中矩阵的转置，除了`np.linalg.eig`返回的特征向量矩阵。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "def get_mean(_X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    计算m_i（样本均值）\n",
    "    :param _X: 样本矩阵，每一行是一个样本\n",
    "    :return: 均值（一维向量）\n",
    "    \"\"\"\n",
    "    return np.mean(_X, 0)\n",
    "\n",
    "\n",
    "def get_within_class_scatter_matrix(_X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    计算某一类的S_w_i（类内离散度矩阵）\n",
    "    :param _X: 样本矩阵，每一行是一个样本\n",
    "    :return: 类内离散度矩阵（D * D）\n",
    "    \"\"\"\n",
    "    ret = np.zeros((_X.shape[1], _X.shape[1]))\n",
    "    m = get_mean(_X)\n",
    "    for row in _X:\n",
    "        ret += (row - m).reshape(m.shape[0], 1) @ (row - m).reshape(m.shape[0], 1).T\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_pooled_within_class_scatter_matrix(_X1: np.ndarray, _X2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    计算最终的S_w（总类内离散度矩阵）\n",
    "    :param _X1: 第一类样本矩阵，每一行是一个样本\n",
    "    :param _X2: 第二类样本矩阵，每一行是一个样本\n",
    "    :return: 总类内离散度矩阵（D * D）\n",
    "    \"\"\"\n",
    "    return get_within_class_scatter_matrix(_X1) + get_within_class_scatter_matrix(_X2) / (_X1.shape[0] + _X2.shape[0])\n",
    "\n",
    "def get_between_class_scatter_matrix(_X1: np.ndarray, _X2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    计算S_b（类间离散度矩阵）\n",
    "    :param _X1: 第一类样本矩阵，每一行是一个样本\n",
    "    :param _X2: 第二类样本矩阵，每一行是一个样本\n",
    "    :return: 类间离散度矩阵（D * D）\n",
    "    \"\"\"\n",
    "    n1 = _X1.shape[0]\n",
    "    n2 = _X2.shape[0]\n",
    "    d = _X1.shape[1]\n",
    "    N = n1 + n2\n",
    "    m_1 = get_mean(_X1)\n",
    "    m_2 = get_mean(_X2)\n",
    "    m = get_mean(np.concatenate((_X1, _X2)))\n",
    "    return n1 / N * ((m_1 - m).reshape(d, 1) @ (m_1 - m).reshape(d, 1).T) + \\\n",
    "        n2 / N * ((m_2 - m).reshape(d, 1) @ (m_2 - m).reshape(d, 1).T)\n",
    "\n",
    "def get_W(_X1: np.ndarray, _X2: np.ndarray, d: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    计算最终的W矩阵\n",
    "    :param _X1: 第一类样本矩阵，每一行是一个样本\n",
    "    :param _X2: 第二类样本矩阵，每一行是一个样本\n",
    "    :param d: 最终要投影的特征维度数量\n",
    "    :return: W矩阵\n",
    "    \"\"\"\n",
    "    S_w_inv = np.linalg.inv(get_pooled_within_class_scatter_matrix(_X1, _X2))\n",
    "    S_b = get_between_class_scatter_matrix(_X1, _X2)\n",
    "    c = np.linalg.eig(S_w_inv @ S_b)  # np.np.linalg.eig用来计算特征值和特征向量，返回的结果是复数\n",
    "\n",
    "    # 下面用来对特征值排序，并且记录特征值的序号\n",
    "    eigen_temp = []\n",
    "    for i in range(c[0].shape[0]):\n",
    "        # 找到是实数的特征值\n",
    "        if c[0][i].imag == 0:\n",
    "            eigen_temp.append([c[0][i].real, i])\n",
    "\n",
    "    eigen_temp.sort(reverse=True)\n",
    "\n",
    "    # 下面用来找出最大的d个实特征值的特征向量\n",
    "    ret = []\n",
    "    for i in range(d):\n",
    "        ret.append(c[1][:,eigen_temp[i][1]].real)\n",
    "    return np.array(ret)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W为：\n",
      "[[ 0.16495053 -0.33727579  0.8207165   0.43065159]]\n"
     ]
    }
   ],
   "source": [
    "# 加载数据，这次的数据用sklearn自带的水仙花的数据\n",
    "iris = load_iris()\n",
    "X = iris.data[:100]\n",
    "y = iris.target[:100]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# 寻找两类样本\n",
    "X1 = []\n",
    "X2 = []\n",
    "for i in range(X_train.shape[0]):\n",
    "    if y_train[i] == 0:\n",
    "        X1.append(X_train[i])\n",
    "    else:\n",
    "        X2.append(X_train[i])\n",
    "X1 = np.array(X1)\n",
    "X2 = np.array(X2)\n",
    "\n",
    "# 计算W, d设置为1，即投影后的新特征为1维\n",
    "W = get_W(X1, X2, 1)\n",
    "print(\"W为：\")\n",
    "print(W)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来生成降维后的数据，用支持向量机训练测试一下效果。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "降维之后的样本维度： 1\n",
      "降维之后的正确率： 1.0\n"
     ]
    }
   ],
   "source": [
    "X_train_new = X_train @ W.T\n",
    "X_test_new = X_test @ W.T\n",
    "print(\"降维之后的样本维度：\", X_train_new.shape[1])\n",
    "\n",
    "svm_classifier = SVC(kernel=\"rbf\")  # 用径向基函数作为核函数\n",
    "svm_classifier.fit(X_train_new, y_train)\n",
    "\n",
    "print(\"降维之后的正确率：\", svm_classifier.score(X_test_new, y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "测试结果表明，用基于分类可分性判据的特征提取方法，就算我们把样本从四维降成一维，还能保持100%的正确率。这说明这个降维方法是非常有效的。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 二、主成分分析方法\n",
    "\n",
    "主成分分析是根据方差大小来对样本进行线性变换，最终实现对样本的降维。每一个主成分都是样本协方差矩阵的一个特征值。\n",
    "\n",
    "主成分分析的公式推导并不复杂，书上介绍的也很清楚。\n",
    "\n",
    "主要的问题在于主成分分析没有考虑样本分类信息，只是按照样本方差来处理数据，这样的方法只是把样本降维了，但是不一定对分类有利。\n",
    "\n",
    "主成分分析实现过程和上面说的第一个方法有很多地方重叠，比如都要求特征值和特征向量，根据特征值大小排序，最后通过数学公式进行投影。因此我就不写代码实现了。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 三、K-L变换"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
