{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 第七章、特征选择\n",
    "\n",
    "这一章的目的是根据现有样本的特征，在其中提取出对样本分类有显著影响的特征。\n",
    "\n",
    "前几节介绍了判断某个特征是否对分类有显著影响的数学方法。这里不再赘述了。\n",
    "\n",
    "下面是书中介绍的特征选择的算法。\n",
    "\n",
    "### 一.特征选择的最优算法\n",
    "\n",
    "这个算法是基于深度优先搜索的选择特征算法，通过选取最大化准则函数值进行剪枝，这样可以大大减少搜索次数。但是算法题刷多了就会明白，就算剪枝剪得再多，搜索的复杂度还是指数级别的，还是非常的慢。书上也说了这个算法很慢，通常也没有人会用。\n",
    "\n",
    "### 二、特征选择的次优算法\n",
    "\n",
    "书上介绍了四种算法：单独最优特征的组合、顺序前进法、顺序后退法、增l减r法（l-r法）。四种算法思想都很简单，\n",
    "\n",
    "### 三、特征选择的遗传算法\n",
    "\n",
    "遗传算法是通过模拟生物染色体遗传进化来实现特征选择，在特征维数很高的时候效果很好。这个算法应用非常广，但是书上也只是简单介绍了一下。\n",
    "\n",
    "### 四、以分类性能为准则的特征选择方法\n",
    "\n",
    "这个方法的思路就是用分类器进行特征选择，书上介绍了R-SVM和SVM-RFE方法。\n",
    "\n",
    "这两个方法是利用了支持向量机在样本维数很高并且样本较少的情况下表现很好的特性，通过SVM训练之后的权值数据来判断特征对分类贡献大小。\n",
    "\n",
    "具体的方法如下：\n",
    "\n",
    "1. 用**线性**支持向量机对样本数据进行训练，得到对应对偶问题的$\\boldsymbol{\\alpha}$\n",
    "2. 通过$\\boldsymbol{\\alpha}$算出原始问题的系数$\\mathbf{w}$\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = \\sum_{i=1}^N\\alpha_iy_i\\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "说明一下，sklearn中的线性支持向量机LinearSVC中有直接算出$\\mathbf{w}$的API，不用先计算出$\\boldsymbol{\\alpha}$，下面的代码中我会直接用这个API。\n",
    "\n",
    "3. 计算每个特征对分类的贡献。对R-SVM和SVM-RFE来说，分类的贡献公式分别是：\n",
    "\n",
    "$$\\begin{align}\n",
    "s_j^{\\text{R-SVM}} &= w_j(m_j^+ - m_j^-) \\\\\n",
    "s_j^{\\text{SVM-RFE}} &= w_j^2 \\quad j = 1,\\cdots,d\n",
    "\\end{align}$$\n",
    "\n",
    "4. 接下来就根据不同特征的贡献选择特征\n",
    "\n",
    "实现代码如下：\n",
    "\n",
    "首先导包并导入数据（数据用之前用过的手写数字识别）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小如下：\n",
      "(800, 400)\n",
      "(800,)\n",
      "测试集大小如下：\n",
      "(200, 400)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.load(\"./data/digit_0_1_X.npy\")\n",
    "y = np.load(\"./data/digit_0_1_y.npy\")\n",
    "y = y.reshape(y.shape[0])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:1000], y[:1000], test_size=0.2)\n",
    "\n",
    "print(\"训练集大小如下：\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\"测试集大小如下：\")\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "接下来用线性支持向量机训练上述数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练后的w大小为： (400,)\n",
      "在测试集上正确率为： 0.995\n"
     ]
    }
   ],
   "source": [
    "linear_SVC = LinearSVC()\n",
    "\n",
    "linear_SVC.fit(X_train, y_train)\n",
    "\n",
    "w = linear_SVC.coef_.reshape(X_train[0].shape[0])\n",
    "\n",
    "print(\"训练后的w大小为：\", w.shape)\n",
    "\n",
    "print(\"在测试集上正确率为：\", linear_SVC.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "接下来计算不同特征的贡献。书上说R-SVM更好，所以下面计算R-SVM的贡献。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s的大小为： (400,)\n",
      "s的最大值为： 0.00724647354728087 \n",
      "s的最小值为： -0.01821647548504886\n"
     ]
    }
   ],
   "source": [
    "m_1 = np.lib.average(X_train[:50], 0)\n",
    "m_2 = np.lib.average(X_train[50:], 0)\n",
    "\n",
    "s = w * (m_1 - m_2)\n",
    "\n",
    "print(\"s的大小为：\", s.shape)\n",
    "print(\"s的最大值为：\", np.max(s), \"\\ns的最小值为：\", np.min(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "下面删掉贡献值最小的200个特征特征（总共有400个特征），再重新训练测试正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "删除后的训练集大小： (800, 200)\n",
      "删除后的测试集大小： (200, 200)\n",
      "删除两百个特征后的正确率： 0.995\n"
     ]
    }
   ],
   "source": [
    "# 统计贡献值最小的两百个特征\n",
    "# 下面的这段代码可能不太容易看懂，不懂也没关系，只需要知道作用就行了\n",
    "s_temp = np.concatenate((s.reshape(s.shape[0], 1), np.arange(0, s.shape[0]).reshape(s.shape[0], 1)), 1)\n",
    "s_temp.sort(0)\n",
    "index_to_delete = []\n",
    "for row in s_temp:\n",
    "    index_to_delete.append(int(row[1]))\n",
    "\n",
    "# 删掉贡献值最小的两百个特征\n",
    "X_train = np.delete(X_train, index_to_delete[:200], axis=1)\n",
    "X_test = np.delete(X_test, index_to_delete[:200], axis=1)\n",
    "\n",
    "print(\"删除后的训练集大小：\", X_train.shape)\n",
    "print(\"删除后的测试集大小：\", X_test.shape)\n",
    "\n",
    "linear_SVC.fit(X_train, y_train)\n",
    "\n",
    "print(\"删除两百个特征后的正确率：\", linear_SVC.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "很明显删除了一半的特征之后正确率还是没有变化，这说明我们使用的特征选择方法是很有效的。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
