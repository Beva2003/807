{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第八章 特征提取\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二轮总结笔记\n",
    "\n",
    "### 一、基于类别可分性判据的特征提取（线性）\n",
    "\n",
    "#### 1. 基于类内类间距离的可分性判据算法\n",
    "\n",
    "计算$\\mathbf{S}_\\text{w}^{-1}\\mathbf{S}_\\text{b}$的特征值和特征向量，选择最大的$d$个特征值的特征向量构成$D\\times d$维线性变换阵\n",
    "\n",
    "$$\\mathbf{W}=[\\mathbf{u}_1,\\mathbf{u}_2,\\cdots,\\mathbf{u}_d]$$\n",
    "\n",
    "则变换后的$d$维新特征为\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{W}^\\text{T}\\mathbf{x}\n",
    "$$\n",
    "\n",
    "其中，$\\mathbf{S}_\\text{w}$和$\\mathbf{S}_\\text{b}$为\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{S}_\\text{w} &= \\sum_{i=1}^cP_i\\frac{1}{n_i}\\sum_{k=1}^{n_i}(\\mathbf{x}_k^{(i)}-\\mathbf{m}_i)(\\mathbf{x}_k^{(i)}-\\mathbf{m}_i)^\\text{T} \\\\\n",
    "\\mathbf{S}_\\text{b} &= \\sum_{i=1}^cP_i(\\mathbf{m}_i-\\mathbf{m})(\\mathbf{m}_i-\\mathbf{m})^\\text{T}\n",
    "\\end{aligned}$$\n",
    "\n",
    "#### 2. 性质\n",
    "\n",
    "1. 采用不同形式的类内类间距离可分性判据，得到的最优变换矩阵相同。\n",
    "2. 采用基于概率距离或基于熵的判据作为准则时一般只能求数值解，只有数据服从正态分布且满足某些特殊条件时才能得到解析解。\n",
    "\n",
    "\n",
    "### 二、主成分分析方法（principal component analysis，PCA）（线性）\n",
    "\n",
    "#### 1. 思想\n",
    "\n",
    "求解最优的正交变换$\\mathbf{A}$，使新特征$\\xi_i$的方差最大。正交变换保证新特征间不相关，新特征方差越大，则样本在该维特征上的差异越大，因而这一特征越重要。\n",
    "\n",
    "#### 2. 算法\n",
    "\n",
    "1. 求协方差矩阵，协方差矩阵可以用样本估计\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Sigma}=\\frac{1}{N}\\sum_{i=1}^N(\\mathbf{x}_i-\\mathbf{m})(\\mathbf{x}_i-\\mathbf{m})^\\text{T} = \\frac{1}{N}\\mathbf{X}^\\text{T}\\mathbf{X} - \\mathbf{mm}^\\text{T}\n",
    "$$\n",
    "\n",
    "2. 求$\\boldsymbol{\\Sigma}$的特征值和**正交归一**的特征向量。\n",
    "3. 选取$\\boldsymbol{\\Sigma}$特征值最大的$d$个**正交归一**的特征向量，组成变换阵$\\mathbf{A}=[\\mathbf{u}_1,\\mathbf{u}_2,\\cdots,\\mathbf{u}_d]$。\n",
    "4. 零均值化变换后的新特征为$\\boldsymbol{\\xi}=\\mathbf{A}^\\text{T}(\\mathbf{x}-\\boldsymbol{\\mu})$，从$\\boldsymbol{\\xi}$到$\\mathbf{x}$的逆变换为$\\mathbf{x}=\\mathbf{A}\\boldsymbol{\\xi}+\\boldsymbol{\\mu}$\n",
    "\n",
    "#### 3. 性质\n",
    "\n",
    "1. 新特征是原有特征的线性组合，且相互之间是不相关的。\n",
    "2. 第$i$个**新特征**$\\xi_i$叫**第$i$主成分**，第$i$主成分的**方差**为第$i$大的特征值$\\lambda_i$。\n",
    "3. 前$k$个主成分代表的数据方差与全部方差的比例为$\\displaystyle \\frac{\\displaystyle\\sum_{i=1}^k\\lambda_i}{\\displaystyle\\sum_{i=1}^p\\lambda_i}$，数据中大部分信息集中在较少的几个主成分上。\n",
    "4. 排列在后面的主成分（次成分）反映了数据中的**随机噪声**，可以通过将次成分置为0再变换为原空间实现降噪。\n",
    "5. 主成分分析是非监督的，没有考虑样本类别信息，不一定有利于分类。\n",
    "\n",
    "\n",
    "### 三、K-L变换（线性）\n",
    "\n",
    "#### 1. 思想\n",
    "\n",
    "##### (1) K-L展开\n",
    "\n",
    "用**完备**的**正交归一**的向量系$\\mathbf{u}_j$对新向量$\\hat{\\mathbf{x}}$展开\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}} = \\sum_{i=1}^d c_j\\mathbf{u}_j\n",
    "$$\n",
    "\n",
    "##### (2) K-L变换\n",
    "\n",
    "1. 步骤与主成分分析类似。\n",
    "2. K-L变换的思想是最小化$\\hat{\\mathbf{x}}$与$\\mathbf{x}$的均方误差。\n",
    "3. 产生矩阵是$\\mathbf{x}$的二阶矩阵$\\displaystyle\\boldsymbol\\Psi=E[\\mathbf{xx}^\\text{T}]=\\frac{1}{N}\\mathbf{X}^\\text{T}\\mathbf{X}$\n",
    "4. $\\mathbf{u}_j$组成新的特征空间，$c_j=\\mathbf{u}_j\\mathbf{x}$组成新的特征向量。\n",
    "\n",
    "#### 2. 与主成分分析对比\n",
    "\n",
    "1. K-L变换的产生矩阵是$\\mathbf{x}$的**二阶矩阵**$\\boldsymbol\\Psi=E[\\mathbf{xx}^\\text{T}]$，主成分分析的产生矩阵是$\\mathbf{x}$的**协方差矩阵**$\\Sigma=E[\\mathbf{xx}^\\text{T}]-E[\\mathbf{x}]E[\\mathbf{x}^\\text{T}]$。\n",
    "2. 主成分分析的原理是新特征**方差最大**，K-L变换的原理是新向量与原向量的**均方误差最小**。\n",
    "3. 当原特征为零均值或对原特征进行**去均值**处理时，主成分分析与K-L变换**等价**，这种非监督的特征提取方法也叫**SELFIC方法**。\n",
    "\n",
    "#### 3. 性质\n",
    "\n",
    "1. K-L变换是正交归一变换。\n",
    "2. K-L变换是信号的**最佳压缩**表示，用$d$维K-L变换特征代表原始样本带来的误差在所有$d$维正交坐标变换中最小。\n",
    "3. K-L变换的新特征是**互不相关**的，新特征向量的二阶矩阵是对角阵，对角线元素就是K-L变换中的特征值。\n",
    "4. K-L变换表示原数据，**表示熵最小**，即在这种坐标系统下，样本的方差信息最大程度的集中在了较少的维度上。\n",
    "5. 如果用特征值**最小**的K-L变换坐标来表示原数据，则**总体熵最小**，即在这些坐标上的均值能最好的代表样本集。\n",
    "\n",
    "#### 4. 用于监督模式识别的K-L变换\n",
    "\n",
    "##### (1) 从类均值中提取判别信息\n",
    "\n",
    "1. 思想：\n",
    "\n",
    "如果样本中的主要分类信息包含在**均值**中，先用$\\mathbf{S}_\\text{w}$做K-L变换消除特征间相关性，然后考虑变换后特征的类均值和方差，选择**方差小**，**类均值与总体均值差别大**的特征。\n",
    "\n",
    "2. 算法：\n",
    "\n",
    "+ 用$\\mathbf{S}_\\text{w}$做K-L变换得到新特征$y_i=\\mathbf{u}_i^\\text{T}\\mathbf{x}$，各维新特征的方差是$\\lambda_i$。\n",
    "+ 计算新特征的分类性能指标，选择最大的$d$个特征，用相应的特征向量$\\mathbf{u}_i$组成变换矩阵$\\mathbf{U}=[\\mathbf{u}_1,\\mathbf{u}_2,\\cdots,\\mathbf{u}_d]$。\n",
    "\n",
    "其中，分类性能指标为\n",
    "\n",
    "$$\n",
    "J(y_i)=\\frac{\\mathbf{u}_i^\\text{T}\\mathbf{S}_\\text{b}\\mathbf{u}_i}{\\lambda_i},\\quad i=1,\\cdots,D\n",
    "$$\n",
    "\n",
    "##### (2) 包含在类平均向量中判别信息的最优压缩\n",
    "\n",
    "1. 思想：\n",
    "\n",
    "在使特征间**互不相关**的前提下最优压缩**均值向量**中包含的**分类信息**。\n",
    "\n",
    "2. 算法：\n",
    "\n",
    "+ 用$\\mathbf{S}_\\text{w}$做K-L变换，得到特征值和特征矩阵，即$\\mathbf{U}^\\text{T}\\mathbf{S}_\\text{w}\\mathbf{U}=\\boldsymbol\\Lambda$。\n",
    "+ 令$\\mathbf{B}=\\mathbf{U}\\boldsymbol\\Lambda^{-\\frac{1}{2}}$，这一步称为**白化**变化，保证再进行正交归一变换后$\\mathbf{S}_\\text{w}$不变。白化后$\\mathbf{S}_\\text{w}'=\\mathbf{I}$，$\\mathbf{S}_\\text{b}'=\\mathbf{B}^\\text{T}\\mathbf{S}_\\text{b}\\mathbf{B}$。\n",
    "+ 用$\\mathbf{S}_\\text{b}'$做K-L变换，得到的变换矩阵最多有$d=c-1$维：$\\mathbf{V}'=[\\mathbf{v}_1,\\mathbf{v}_2,\\cdots,\\mathbf{v}_d]$\n",
    "+ 最终总的变换矩阵为$\\mathbf{W}=\\mathbf{U}\\boldsymbol\\Lambda^{-\\frac{1}{2}}\\mathbf{V}'$\n",
    "\n",
    "3. 性质：两类情况下$\\mathbf{w}$等价于Fisher线性判别投影方向。\n",
    "\n",
    "##### (3) 类中心化特征向量中分类信息的提取\n",
    "\n",
    "1. 思想：\n",
    "\n",
    "各类样本减去各自均值后消除了各类均值差别包含的分类信息，此时如果各类分布形状不同，仍然可以从各类的**协方差**中提取出分类信息。\n",
    "\n",
    "2. 算法：\n",
    "\n",
    "+ 用$\\mathbf{S}_\\text{w}$做K-L变换，得到新的特征。\n",
    "+ 算出在各类样本中新特征的方差，$\\omega_i$类的第$j$个特征的方差为$r_{ij}$，并用第$j$个特征总的方差$\\lambda_j$和先验概率进行归一化\n",
    "\n",
    "$$\n",
    "\\tilde{r}_{ij} = P(\\omega_i)\\frac{r_{ij}}{\\lambda_j},\\quad i=1,\\cdots,c,\\quad j=1,\\cdots,D\n",
    "$$\n",
    "\n",
    "+ 定义总体熵表示方差的分散程度\n",
    "\n",
    "$$\n",
    "J(x_j)=-\\sum_{i=1}^c\\tilde{r}_{ij}\\log\\tilde{r}_{ij}\\quad或\\quad J(x_j)=\\prod_{i=1}^c\\tilde{r}_{ij}\n",
    "$$\n",
    "\n",
    "$J(x_j)$越**小**表明方差中包含的分类信息越大，这个特征越重要。\n",
    "\n",
    "+ 选取$J(x_j)$**最小**的$d$个特征组成新的特征。\n",
    "\n",
    "##### (4) 三个方法对比\n",
    "\n",
    "1. 共同点：\n",
    "\n",
    "都要首先用$\\mathbf{S}_\\text{w}$做K-L变换，用来消除特征间相关性。\n",
    "\n",
    "2. 前两个和第三个的区别：\n",
    "\n",
    "前两个只考虑了不同类均值中的分类信息，第三个只考虑了不同类分布形状（协方差）中的分类信息。\n",
    "\n",
    "3. 第二个和其他两个的区别：\n",
    "\n",
    "第二个先对不同类样本进行白化（统一不同类的分布形状和协方差），再进行K-L变换，有两次变换。\n",
    "\n",
    "第一个和第三个直接对不同类样本用$\\mathbf{S}_\\text{w}$做K-L变换，只有一次变换，其他步骤只是按照指标挑选出变换的向量。\n",
    "\n",
    "4. 使用方式：\n",
    "\n",
    "用第二个方法得到$d'\\leqslant c-1$个特征，再用第三个方法得到另外$d-d'$个特征。这样及考虑到了样本均值中分类信息，也考虑到了样本方差中的分类信息。\n",
    "\n",
    "\n",
    "### 四、高维数据的低维表示（线性或非线性）\n",
    "\n",
    "#### 1. 多维尺度法（MDS/ALSCAL）的基本概念\n",
    "\n",
    "##### (1) 思想\n",
    "\n",
    "把样本之间的**距离关系**或**不相似关系**在二维或三维空间中表示出来。\n",
    "\n",
    "##### (2) 分类\n",
    "\n",
    "1. 度量型：把距离或不相似度看作**定量**的度量，在低维空间中保持度量关系。\n",
    "2. 非度量型：只要保持样本间距离和不相似度**定性**的关系顺序。\n",
    "\n",
    "#### 2. 古典尺度法（Classical Scaling）/主坐标分析（principal coordinates analysis）\n",
    "\n",
    "##### (1) 思想\n",
    "\n",
    "已知样本间欧氏距离，求样本坐标。\n",
    "\n",
    "##### (2) 算法\n",
    "\n",
    "1. 定义中心化矩阵：\n",
    "\n",
    "$$\n",
    "\\mathbf{J} = \\mathbf{I} - \\frac{1}{n}\\mathbf{11}^\\text{T}\n",
    "$$\n",
    "\n",
    "2. 用欧氏距离矩阵计算样本内积矩阵（双中心化）：\n",
    "\n",
    "$$\n",
    "\\mathbf{B} = \\mathbf{XX}^\\text{T} = -\\frac{1}{2}\\mathbf{JD}^{(2)}\\mathbf{J}\n",
    "$$\n",
    "\n",
    "3. 对内积矩阵进行特征值分解：\n",
    "\n",
    "$$\n",
    "\\mathbf{B} = \\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{U}^\\text{T}\n",
    "$$\n",
    "\n",
    "4. 解出样本坐标：\n",
    "\n",
    "+ 如果全体样本是零均值的，则\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\mathbf{U}\\boldsymbol\\Lambda^{\\frac{1}{2}}\n",
    "$$\n",
    "\n",
    "+ 如果不是零均值，则\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{x}}_i = \\mathbf{x}_i + \\bar{\\mathbf{x}},\\quad i=1,\\cdots,n\n",
    "$$\n",
    "\n",
    "+ 如果要用$k<d$维空间表示样本，则把特征值从大到小排序，用前$k$个特征值和特征向量组成$\\boldsymbol\\Lambda$和$\\mathbf{U}$。\n",
    "\n",
    "##### (3) 性质\n",
    "\n",
    "如果已知样本集，从中计算出$\\mathbf{D}^{(2)}$，再用古典尺度法得到样本的低维表示，结果与主成分分析相同。\n",
    "\n",
    "#### 3. 度量型MDS\n",
    "\n",
    "##### (1) 思想\n",
    "\n",
    "样本在低维空间中两两之间的距离$d_{ij}$尽可能忠实的代表样本之间的相异度度量$\\delta_{ij}$。\n",
    "\n",
    "##### (2) 压力函数（stress function）\n",
    "\n",
    "1. 定义：表示给定距离和表示距离之间的误差的目标函数。\n",
    "2. 常用压力函数：\n",
    "+ $S = \\displaystyle \\sum_{ij}^\\infty(\\delta_{ij}^2-d_{ij}^2)$，即给定距离和表示距离间的平方之间的平均误差，当$\\delta_{ij}$是欧氏距离时，得到的低维空间表示就是样本在**主成分方向**的投影。\n",
    "+ $\\displaystyle S = \\sum_{ij}\\alpha_{ij}(\\phi(\\delta_{ij})-d_{ij})^2$，$\\alpha_{ij}$是对样本对的加权，比如$\\displaystyle \\alpha_{ij}=\\left(\\sum_{ij}^{\\infty}d_{ij}^2\\right)^{-1}$。\n",
    "+ $\\displaystyle S = \\sqrt{\\displaystyle \\frac{\\displaystyle \\sum_{i,j}(f(\\delta_{ij})-d_{ij})^2}{\\text{scale}}}$，$\\text{scale}$是尺度因子，比如$\\displaystyle\\text{scale}=\\sum_{i,j}\\delta_{ij}^2$，此时压力函数称作Kruskal压力（Kruskal stress）。\n",
    "\n",
    "##### (3) 算法\n",
    "\n",
    "1. 如果$\\phi()$或$f()$已经确定，采用迭代优化算法。\n",
    "2. 如果$\\phi()$或$f()$中有待定参数，采用交替最小二乘的方法进行优化，即固定坐标优化参数，固定参数优化坐标。\n",
    "\n",
    "#### 4. 非度量型MDS（顺序MDS）\n",
    "\n",
    "##### (1) 思想\n",
    "\n",
    "用低维空间坐标表示样本间的距离关系，尽可能接近的反应原相异度矩阵表示的的顺序关系。\n",
    "\n",
    "##### (2) 算法\n",
    "\n",
    "也需要最小化压力函数，但是$\\phi()$或$f()$只需要是**单调函数**或者**弱单调函数**即可，可以通过**单调回归（monotonic regression）**实现。\n",
    "\n",
    "#### 5. MDS在模式识别中的应用\n",
    "\n",
    "##### (1) 陡坡图（stee plot）\n",
    "\n",
    "横轴为维数，纵轴为最优压力函数取值。单调下降，往往有拐点，但是不是所有数据都会有明显拐点。\n",
    "\n",
    "##### (2) 特征变换\n",
    "\n",
    "在陡坡图找到拐点，确定恰当的MDS维数。\n",
    "\n",
    "\n",
    "### 五、非线性变换方法\n",
    "\n",
    "#### 1. 核主成分分析（KPCA）\n",
    "\n",
    "##### (1) 思想\n",
    "\n",
    "对样本进行非线性变换，通过在变换空间进行主成分分析实现原空间的非线性主成分分析。\n",
    "\n",
    "##### (2) 算法\n",
    "\n",
    "1. 计算核函数矩阵$\\mathbf{K}=\\{k(\\mathbf{x}_i,\\mathbf{x}_j)\\}_{n\\times n}$。\n",
    "2. 对$\\mathbf{K}$进行特征值分解得到第$l$大的特征值的特征向量$\\boldsymbol\\alpha^l = [\\alpha_1^l,\\alpha_2^l,\\cdots,\\alpha_n^l]^\\text{T}$。\n",
    "3. 计算出样本在第$l$非线性主成分方向的投影（新特征）$\\displaystyle z^l(\\mathbf{x})=\\sum_{i=1}^n\\alpha_i^lk(\\mathbf{x}_i,\\mathbf{x})$，保留前$k$个。\n",
    "\n",
    "##### (3) 性质\n",
    "\n",
    "由于引入非线性变换，得到的非零特征值可能超过样本原来的维数。\n",
    "\n",
    "\n",
    "### 六、IsoMap方法和LLE方法\n",
    "\n",
    "#### 1. 思想\n",
    "\n",
    "通过局部距离定义非线性距离度量，在样本分布比较**密集**的情况下可以实现复杂的非线性距离度量。\n",
    "\n",
    "#### 2. IsoMap方法（isometric feature mapping）\n",
    "\n",
    "##### (1) 算法\n",
    "\n",
    "1. 把**相邻样本**看作直接相连的节点，边长是两个相邻样本的欧氏距离，不相邻样本间没有边。对于不相邻的样本，计算最短路，用最短路长度作为距离度量（测地距离）。\n",
    "2. 有了样本间距离矩阵，可以使用MDS映射到低维空间。\n",
    "\n",
    "#### 3. LLE方法（locally linear embedding）\n",
    "\n",
    "##### (1) 算法\n",
    "\n",
    "1. 在原空间中，对样本$\\mathbf{x}_i$选择一组**邻域样本**。\n",
    "2. 用这一组邻域样本的线性加权组合重构$x$，得到一组使重构误差$\\displaystyle |\\mathbf{x}_i - \\sum_jw_{ij}\\mathbf{x}_j|$最小的权值$w_{ij}$。\n",
    "3. 在低维空间求向量$\\mathbf{y}_i$及其邻域的映射，使得对所有样本用同样的权值进行重构得到的误差$\\displaystyle |\\mathbf{y}_i-\\sum_jw_{ij}\\mathbf{y}_j|$最小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 一轮学习笔记（包含代码实现）\n",
    "\n",
    "本章的特征提取指的是特征变换。\n",
    "\n",
    "特征变换和前面的特征选择的目的都是将特征降维，但是特征选择是直接删掉一些特征，而特征提取是通过数学方法把高维特征映射为低维。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、基于类别可分性判据的特征提取\n",
    "\n",
    "这一节介绍的方法是通过最大化类内类间可分性判据$J$来对特征进行线性变换降维，也就是找到一个$\\mathbf{W}$，将样本$\\mathbf{x}$投影为$\\mathbf{y} = \\mathbf{W}^\\text{T}\\mathbf{x}$，用投影后的$\\mathbf{y}$算出来的可分性判据$J$最大。\n",
    "\n",
    "书上的推导过程需要先学会标量对矩阵求导，我在[第00章 补充的数学知识.ipynb](./第00章%20补充的数学知识.ipynb)中有介绍。\n",
    "\n",
    "经过一系列推导，最终的方法是：\n",
    "\n",
    "1. 求出矩阵$\\mathbf{S}_{\\text{w}}^{-1}\\mathbf{S}_{\\text{b}}$。\n",
    "\n",
    "关于$\\mathbf{S}_{\\text{w}}$和$\\mathbf{S}_{\\text{b}}$，书上在第四章和第七章中的表述不一致，我认为应该这里的公式应该采取第七章的说法，即\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{S}_{\\text{b}} &= \\sum_{i=1}^cP_i(\\mathbf{m}_i - \\mathbf{m})(\\mathbf{m}_i - \\mathbf{m})^\\text{T} \\\\\n",
    "\\mathbf{S}_{\\text{w}} &= \\sum_{i=1}^cP_i\\frac{1}{n_i}\\sum_{k=1}^{n_i}(\\mathbf{x}_k^{(i)}-\\mathbf{m}_i)(\\mathbf{x}_k^{(i)}-\\mathbf{m}_i)^\\text{T}\n",
    "\\end{align}$$\n",
    "\n",
    "其中$P_i$采用样本中类别的比例来进行估算，其实如果这样算的话公式里的$\\displaystyle P_i\\frac{1}{n_i}$就变成了$\\displaystyle \\frac{1}{n}$。\n",
    "\n",
    "2. 求出$\\mathbf{S}_{\\text{w}}^{-1}\\mathbf{S}_{\\text{b}}$的特征值和特征向量。\n",
    "3. 把特征值从大到小排序，选取前面最大的$d$个特征值的特征向量作为$\\mathbf{W}$。$d$是人为划定的，你想把样本降维成几维，$d$就是多大。\n",
    "\n",
    "计算特征值可以用`np.linalg.eig`方法来实现。`np.linalg.eig`返回值包含两部分，特征值和每个特征值对应的列向量（注意这里是列向量）。\n",
    "\n",
    "接下来是代码实现，还是注意和前几章同样的问题，numpy中向量是行向量，代码中实现的矩阵是公式中矩阵的转置，除了`np.linalg.eig`返回的特征向量矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean(_X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    计算m_i（样本均值）\n",
    "    :param _X: 样本矩阵，每一行是一个样本\n",
    "    :return: 均值（一维向量）\n",
    "    \"\"\"\n",
    "    return np.mean(_X, 0)\n",
    "\n",
    "\n",
    "def get_within_class_scatter_matrix(_X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    计算某一类的S_w_i（类内离散度矩阵）\n",
    "    :param _X: 样本矩阵，每一行是一个样本\n",
    "    :return: 类内离散度矩阵（D * D）\n",
    "    \"\"\"\n",
    "    ret = np.zeros((_X.shape[1], _X.shape[1]))\n",
    "    m = get_mean(_X)\n",
    "    for row in _X:\n",
    "        ret += (row - m).reshape(m.shape[0], 1) @ (row - m).reshape(m.shape[0], 1).T\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_pooled_within_class_scatter_matrix(_X1: np.ndarray, _X2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    计算最终的S_w（总类内离散度矩阵）\n",
    "    :param _X1: 第一类样本矩阵，每一行是一个样本\n",
    "    :param _X2: 第二类样本矩阵，每一行是一个样本\n",
    "    :return: 总类内离散度矩阵（D * D）\n",
    "    \"\"\"\n",
    "    return get_within_class_scatter_matrix(_X1) + get_within_class_scatter_matrix(_X2) / (_X1.shape[0] + _X2.shape[0])\n",
    "\n",
    "def get_between_class_scatter_matrix(_X1: np.ndarray, _X2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    计算S_b（类间离散度矩阵）\n",
    "    :param _X1: 第一类样本矩阵，每一行是一个样本\n",
    "    :param _X2: 第二类样本矩阵，每一行是一个样本\n",
    "    :return: 类间离散度矩阵（D * D）\n",
    "    \"\"\"\n",
    "    n1 = _X1.shape[0]\n",
    "    n2 = _X2.shape[0]\n",
    "    d = _X1.shape[1]\n",
    "    N = n1 + n2\n",
    "    m_1 = get_mean(_X1)\n",
    "    m_2 = get_mean(_X2)\n",
    "    m = get_mean(np.concatenate((_X1, _X2)))\n",
    "    return n1 / N * ((m_1 - m).reshape(d, 1) @ (m_1 - m).reshape(d, 1).T) + \\\n",
    "        n2 / N * ((m_2 - m).reshape(d, 1) @ (m_2 - m).reshape(d, 1).T)\n",
    "\n",
    "def get_W(_X1: np.ndarray, _X2: np.ndarray, d: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    计算最终的W矩阵\n",
    "    :param _X1: 第一类样本矩阵，每一行是一个样本\n",
    "    :param _X2: 第二类样本矩阵，每一行是一个样本\n",
    "    :param d: 最终要投影的特征维度数量\n",
    "    :return: W矩阵\n",
    "    \"\"\"\n",
    "    S_w_inv_ = np.linalg.inv(get_pooled_within_class_scatter_matrix(_X1, _X2))\n",
    "    S_b_ = get_between_class_scatter_matrix(_X1, _X2)\n",
    "\n",
    "    # np.np.linalg.eig用来计算特征值和特征向量，返回的结果可能是复数\n",
    "    eigen_values_, eigen_vectors_ = np.linalg.eig(S_w_inv_ @ S_b_)\n",
    "\n",
    "    # 下面用来对特征值排序，并且记录特征值的序号\n",
    "    eigen_temp_ = []\n",
    "    for i in range(eigen_values_.shape[0]):\n",
    "        # 找到是实数的特征值\n",
    "        if eigen_values_[i].imag == 0:\n",
    "            eigen_temp_.append([eigen_values_[i].real, i])\n",
    "\n",
    "    eigen_temp_.sort(reverse=True)\n",
    "\n",
    "    # 下面用来找出最大的d个实特征值的特征向量\n",
    "    ret = []\n",
    "    for i in range(d):\n",
    "        ret.append(eigen_vectors_[:,eigen_temp_[i][1]].real)\n",
    "    return np.array(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据，这次的数据用sklearn自带的水仙花的数据\n",
    "iris = load_iris()\n",
    "X = iris.data[:100]\n",
    "y = iris.target[:100]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# 寻找两类样本\n",
    "X1 = []\n",
    "X2 = []\n",
    "for i in range(X_train.shape[0]):\n",
    "    if y_train[i] == 0:\n",
    "        X1.append(X_train[i])\n",
    "    else:\n",
    "        X2.append(X_train[i])\n",
    "X1 = np.array(X1)\n",
    "X2 = np.array(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W为：\n",
      "[[ 0.01869426 -0.2069536   0.92145113  0.32825073]]\n"
     ]
    }
   ],
   "source": [
    "# 计算W, d设置为1，即投影后的新特征为1维\n",
    "W = get_W(X1, X2, 1)\n",
    "print(\"W为：\")\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来生成降维后的数据，用支持向量机训练测试一下效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "降维之后的样本维度： 1\n",
      "降维之后的正确率： 1.0\n"
     ]
    }
   ],
   "source": [
    "X_train_new = X_train @ W.T\n",
    "X_test_new = X_test @ W.T\n",
    "print(\"降维之后的样本维度：\", X_train_new.shape[1])\n",
    "\n",
    "svm_classifier = SVC(kernel=\"rbf\")  # 用径向基函数作为核函数\n",
    "svm_classifier.fit(X_train_new, y_train)\n",
    "\n",
    "print(\"降维之后的正确率：\", svm_classifier.score(X_test_new, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试结果表明，用基于分类可分性判据的特征提取方法，就算我们把样本从四维降成一维，还能保持100%的正确率。这说明这个降维方法是非常有效的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、主成分分析方法\n",
    "\n",
    "主成分分析是根据方差大小来对样本进行线性变换，最终实现对样本的降维。每一个主成分都是样本协方差矩阵的一个特征值。\n",
    "\n",
    "主成分分析的公式推导并不复杂，书上介绍的也很清楚。\n",
    "\n",
    "主要的问题在于主成分分析没有考虑样本分类信息，只是按照样本方差来处理数据，这样的方法只是把样本降维了，但是不一定对分类有利。\n",
    "\n",
    "主成分分析实际上是K-L变换的一种特殊情况，下面会实现K-L变换，因此这里我就不写代码实现了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三、K-L变换\n",
    "\n",
    "原始的K-L变换的根据最小均方误差来进行线性变换，如果样本均值为$0$，那么K-L变换等价于主成分分析。\n",
    "\n",
    "为了应用于模式识别，需要考虑到样本的分类信息，K-L变换引入了新的方法。\n",
    "\n",
    "#### 1. 从类均值中提取判别信息\n",
    "\n",
    "这个方法是假设样本的主要分类信息包含在均值中，通过总类内离散度矩阵$\\mathbf{S}_{\\text{w}}$作为产生矩阵进行K-L变换。然后在根据可分性判据大小得出新特征的转换矩阵。\n",
    "\n",
    "具体方法如下：\n",
    "\n",
    "1. 计算总类内离散度矩阵$\\mathbf{S}_{\\text{w}}$和类间离散度矩阵$\\mathbf{S}_{\\text{b}}$。\n",
    "2. 用$\\mathbf{S}_{\\text{w}}$进行K-L变换，得到特征值$\\lambda_i$和对应的特征向量$\\mathbf{u}_i$（一个特征维度对应一个特征值和特征向量）。\n",
    "3. 通过特征值和特征向量计算样本每个维度的可分性判据大小，公式如下：\n",
    "\n",
    "$$\n",
    "J(y_i) = \\frac{\\mathbf{u}_i^\\text{T}\\mathbf{S}_{\\text{b}}\\mathbf{u}_i}{\\lambda_i}\n",
    "$$\n",
    "\n",
    "4. 找出$J(y_i)$最大的$d$个（d是要降低的目标维数）特征向量$\\mathbf{u}_1,\\mathbf{u}_2,\\cdots, \\mathbf{u}_d$，转换矩阵就是$\\mathbf{U} = [\\mathbf{u}_1,\\mathbf{u}_2,\\cdots, \\mathbf{u}_d]$，降维后的样本就是$\\mathbf{y} = \\mathbf{U}^\\text{T}\\mathbf{x}$。\n",
    "\n",
    "算法实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可分性判据J数值如下： [7.77946014e-03 9.29616393e-01 1.30417535e+00 1.23736680e-03]\n"
     ]
    }
   ],
   "source": [
    "# 数据在上面已经生成过了，这里直接用上面生成的数据\n",
    "S_w = get_pooled_within_class_scatter_matrix(X1, X2)\n",
    "S_b = get_between_class_scatter_matrix(X1, X2)\n",
    "\n",
    "# 注意numpy生成的特征向量矩阵中，每一列是一个特征向量\n",
    "eigen_values, eigen_vectors = np.linalg.eig(S_w)\n",
    "\n",
    "# np.diag把方阵的对角元素提取出来，返回一个向量\n",
    "J = np.diag(eigen_vectors.T @ S_b @ eigen_vectors) / eigen_values\n",
    "print(\"可分性判据J数值如下：\", J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我们要把样本降维成一维，所以转换矩阵U退化成一个向量，U的值为：\n",
      "[ 0.42591017 -0.17061936 -0.82024461 -0.34159675]\n"
     ]
    }
   ],
   "source": [
    "# 接下来找出J最大的d个特征向量，生成转换矩阵U\n",
    "# 这个例子中我们把原来4维的样本降成1维，所以d=1，那么只需要找出最大的J对应的特征向量即可\n",
    "\n",
    "maxn = -999999\n",
    "max_index = -1\n",
    "for i in range(J.shape[0]):\n",
    "    if J[i] > maxn:\n",
    "        maxn = J[i]\n",
    "        max_index = i\n",
    "\n",
    "U = eigen_vectors[:, max_index].reshape(J.shape[0])\n",
    "print(\"我们要把样本降维成一维，所以转换矩阵U退化成一个向量，U的值为：\")\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来把生成降维后的样本，再用SVM测试一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "降维之后的样本维度： 1\n",
      "降维之后的正确率： 1.0\n"
     ]
    }
   ],
   "source": [
    "X_train_new = X_train @ U.reshape(U.shape[0], 1)\n",
    "X_test_new = X_test @ U.reshape(U.shape[0], 1)\n",
    "print(\"降维之后的样本维度：\", X_train_new.shape[1])\n",
    "\n",
    "svm_classifier = SVC(kernel=\"rbf\")  # 用径向基函数作为核函数\n",
    "svm_classifier.fit(X_train_new, y_train)\n",
    "\n",
    "print(\"降维之后的正确率：\", svm_classifier.score(X_test_new, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然，从类均值中提取判别信息的降维效果也很好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 包含在类平均向量中判别信息的最优压缩\n",
    "\n",
    "这个方法是使特征间互不相关的前提下最优压缩均值向量中包含的分类信息。\n",
    "\n",
    "具体方法如下：\n",
    "\n",
    "1. 求出$\\mathbf{S}_{\\text{w}}$和$\\mathbf{S}_{\\text{b}}$\n",
    "2. 对$\\mathbf{S}_{\\text{w}}$做K-L变换，求出特征变换矩阵$\\mathbf{U}$\n",
    "3. 令$\\mathbf{B}=\\mathbf{U}\\boldsymbol{\\Lambda}^{-\\frac{1}{2}}$，这个时候$\\mathbf{B}^\\text{T}\\mathbf{S}_{\\text{w}}\\mathbf{B} = \\mathbf{I}$，这一步称为白化变换\n",
    "4. 求$\\mathbf{S}_{\\text{b}}' = \\mathbf{B}^\\text{T}\\mathbf{S}_{\\text{b}}\\mathbf{B}$\n",
    "5. 对$\\mathbf{S}_{\\text{b}}'$进行K-L变换，求出特征变换矩阵$\\mathbf{V}'$，其中$\\mathbf{V}'$中只包含$\\mathbf{S}_{\\text{b}}'$中非零的实特征值对应的特征向量（$c$类分类问题最多有$c-1$个）\n",
    "6. 最终总的变换矩阵是$\\mathbf{W} = \\mathbf{U}\\boldsymbol{\\Lambda}^{-\\frac{1}{2}}\\mathbf{V}'$\n",
    "\n",
    "最终得到的投影方向实际上就是Fisher线性判别的投影方向。\n",
    "\n",
    "这个方法没有指定到底降维成多少维，我们测试一下实际的降维效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最终计算出的投影方向W为：\n",
      "[[ 0.11277732 -0.23181291 -0.68124047 -0.24026011]\n",
      " [ 0.36302409  0.17933398  0.50943862  0.09630243]\n",
      " [-0.33659659 -0.74372745  0.51266842 -0.42787552]\n",
      " [-0.11711099 -0.28817791  0.05558638  1.52504861]]\n"
     ]
    }
   ],
   "source": [
    "# 注意下面这段代码所有的矩阵都是公式中的矩阵，而不像之前一样是公式中矩阵的转置\n",
    "\n",
    "S_w = get_pooled_within_class_scatter_matrix(X1, X2)\n",
    "S_b = get_between_class_scatter_matrix(X1, X2)\n",
    "eigen_values, eigen_vectors = np.linalg.eig(S_w)\n",
    "U = eigen_vectors\n",
    "Lambda_of_neg_1_2 = np.diag(eigen_values ** (-1 / 2))\n",
    "B = U @ np.linalg.inv(Lambda_of_neg_1_2)\n",
    "S_b_1 = B.T @ S_b @ B\n",
    "eigen_values, eigen_vectors = np.linalg.eig(S_b_1)\n",
    "V_1 = []\n",
    "for i in range(eigen_values.shape[0]):\n",
    "    if eigen_values[i] != 0:\n",
    "        V_1.append(eigen_vectors[:, i].reshape(eigen_vectors.shape[0]))\n",
    "V_1 = np.array(V_1).T\n",
    "W = U @ Lambda_of_neg_1_2 @ V_1\n",
    "print(\"最终计算出的投影方向W为：\")\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然最后样本被降维成三维。这个降维可以理解为保留了样本几乎全部信息，虽然最终维度比较高，但是分类信息保留全面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 类中心化特征向量中分类信息的提取\n",
    "\n",
    "这个方法时考虑从样本的各类协方差中提取信息，具体方式是：先对$\\mathbf{S}_{\\text{w}}$进行K-L变换，然后在根据新样本的各个维度方差的大小来计算可分性判据$J(x_j)$。最后取$J(x_j)$最小的前d个特征。\n",
    "\n",
    "这个方法可以和前面介绍的第二种方法结合起来，先用第二种进行均值分类信息最优压缩，压缩后获得$d' \\leqslant c-1$个特征，再用第三种方法压缩获得另外$d - d'$个特征。\n",
    "\n",
    "算法的实现也和前面的大同小异，这里我就不实现了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四、多维尺度法（MDS）\n",
    "\n",
    "MDS运用了缩放的思想，把高维空间的样本按比例缩放到低维空间，书上用地图的例子很好的解释了这个思想。\n",
    "\n",
    "#### 1. 古典尺度法\n",
    "\n",
    "古典尺度法就是已知各样本间的欧式距离，求他们在指定维度下的坐标。\n",
    "\n",
    "实现方法如下：\n",
    "\n",
    "1. 定义中心化矩阵$\\mathbf{J} = \\mathbf{I} - \\frac{1}{n}\\mathbf{11}^\\text{T}$\n",
    "2. 计算欧氏距离矩阵$\\mathbf{D}^{(2)}$\n",
    "3. 求样本两两内积组成的矩阵$\\mathbf{B} = -\\frac{1}{2}\\mathbf{JDJ}$\n",
    "4. 求$\\mathbf{B}$的特征值$\\lambda_1,\\lambda_2,\\cdots,\\lambda_d$和特征向量$\\mathbf{u}_1,\\mathbf{u}_2,\\cdots,\\mathbf{u}_d$\n",
    "5. 假设要将样本降维到$k$维，则将特征值从大到小排序，取前$k$大的特征值对应的特征向量组成矩阵$\\mathbf{U}=[\\mathbf{u}_1,\\mathbf{u}_2,\\cdots,\\mathbf{u}_k]$\n",
    "6. 解出样本坐标矩阵$\\mathbf{X}=\\mathbf{U}\\boldsymbol{\\Lambda}^{1/2}$\n",
    "7. 将样本从中心化还原，$\\widetilde{\\mathbf{x}}_i = \\mathbf{x}_i + \\overline{\\mathbf{x}}$\n",
    "\n",
    "这个方法其实和主成分分析是相同的，代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.27826921e-08 -3.44144050e-08 -1.75140000e-08  2.28469080e-09\n",
      " -1.67971705e-08  5.34840161e-08  3.43430899e-09  2.98690681e-08\n",
      " -2.13786560e-08  1.63238719e-08  4.11062135e-08 -1.98371979e-08\n",
      " -2.90794537e-08  8.90252228e-09  2.37053957e-09  1.17263691e-08\n",
      "  2.07078762e-08 -9.75305242e-10  3.20626111e-08  2.71718079e-08\n",
      "  3.33402252e-08  1.26129911e-08 -3.12520589e-08 -2.70509339e-09\n",
      " -4.24973096e-08 -1.35221505e-08  7.88039006e-09 -2.87083186e-09\n",
      " -3.48658361e-08 -8.55255398e-09 -9.89458731e-09  2.34282814e-08\n",
      "  1.08033115e-08 -3.47947733e-08 -5.32301673e-09 -5.81293064e-09\n",
      " -2.45663341e-08 -2.86082110e-08 -4.12486016e-08 -8.36352275e-09\n",
      " -1.87246325e-08 -1.01729126e-08 -3.10669694e-08 -1.81945259e-08\n",
      " -1.08101679e-08 -3.65419330e-08 -1.08827192e-08 -4.30372786e-08\n",
      "  4.86084062e-09 -2.70163812e-08  4.15228906e-08 -6.41982072e-09\n",
      " -1.74987220e-08  2.09008665e-08 -2.50100769e-08 -8.61789227e-09\n",
      " -5.92910143e-09 -4.81297694e-09 -6.88161834e-09  4.16202265e-08\n",
      "  1.64202060e-09  1.42302506e-08  5.53318913e-08 -1.16508775e-08\n",
      "  3.62741257e-08 -6.47619201e-08 -1.11632165e-08  2.66905554e-08\n",
      "  2.05870468e-09 -1.41332411e-09 -2.92467205e-08  5.70420669e-09\n",
      " -2.31138382e-08  1.22018194e-08 -4.34116542e-08 -6.20514330e-08\n",
      " -4.79626460e-08 -1.99943528e-08 -3.56117949e-08  1.10452149e-09\n",
      " -2.38777311e-08  2.02512017e-08 -1.46882984e-08 -1.09512754e-08\n",
      " -3.65902431e-08 -2.46556440e-08 -1.87207383e-08 -2.54257930e-08\n",
      "  7.59315658e-10  1.32360475e-10 -1.79835738e-08 -8.77635709e-09\n",
      "  1.72098331e-08  6.08651473e-09  1.19661514e-08  4.29211566e-08\n",
      "  3.15975935e-08  5.21455748e-09  1.43383440e-08  1.35625299e-08]\n"
     ]
    }
   ],
   "source": [
    "n = X.shape[0]\n",
    "J = np.eye(n) - 1 / n * np.ones(n)\n",
    "D = X @ X.T\n",
    "B = -1 / 2 * J @ D @ J\n",
    "eigen_values, eigen_vectors = np.linalg.eig(B)\n",
    "\n",
    "# 下面用来对特征值排序，并且记录特征值的序号\n",
    "eigen_temp = []\n",
    "for i in range(eigen_values.shape[0]):\n",
    "    # 找到是实数的特征值\n",
    "    if eigen_values[i].imag == 0:\n",
    "        eigen_temp.append([eigen_values[i].real, i])\n",
    "eigen_temp.sort(reverse=True)\n",
    "\n",
    "# 下面用来找出最大的d个实特征值的特征向量，假设d=1\n",
    "U = []\n",
    "Lambda_of_1_2 = []\n",
    "for i in range(1):\n",
    "    U.append(eigen_vectors[:,eigen_temp[i][1]].real.T)\n",
    "    Lambda_of_1_2.append(np.sqrt(eigen_temp[i][0].real))\n",
    "U = np.array(U).T\n",
    "Lambda_of_1_2 = np.array(Lambda_of_1_2)\n",
    "X_new = U @ Lambda_of_1_2\n",
    "X_new = X_new + get_mean(X_new)\n",
    "print(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个方法我就不测试了，和前面的都一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 度量型MSD和非度量型MSD\n",
    "\n",
    "这两个书上都只是简单的介绍了一下，看一下就行。这类算法的特点是实际实现没有解析解，要使用梯度下降来计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三、核主成分分析\n",
    "\n",
    "核主成分分析的思想是通过核函数把样本投影到高维，这样原来非线性的规律也变成了线性的，再通过进行线性的主成分分析。\n",
    "\n",
    "具体算法为：\n",
    "\n",
    "1. 计算核函数矩阵$\\mathbf{K}=\\{k(\\mathbf{x}_i,\\mathbf{x}_j)\\}_{n\\times n}$。\n",
    "2. 对$\\mathbf{K}$进行特征值分解得到第$l$大的特征值的特征向量$\\boldsymbol\\alpha^l = [\\alpha_1^l,\\alpha_2^l,\\cdots,\\alpha_n^l]^\\text{T}$。\n",
    "3. 计算出样本在第$l$非线性主成分方向的投影（新特征）$\\displaystyle z^l(\\mathbf{x})=\\sum_{i=1}^n\\alpha_i^lk(\\mathbf{x}_i,\\mathbf{x})$，保留前$k$个。\n",
    "\n",
    "需要注意的是由于引入了非线性变换，特征值分解得到的非零特征值可能会超过样本原来的维数。\n",
    "\n",
    "代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf(_x1: np.ndarray, _x2: np.ndarray, sigma2: float) -> float:\n",
    "    \"\"\"\n",
    "    径向基函数\n",
    "    :param _x1: 第一个向量\n",
    "    :param _x2: 第二个向量\n",
    "    :param sigma2: rbf的sigma参数的平方\n",
    "    :return: 函数值\n",
    "    \"\"\"\n",
    "    return np.exp(-np.sum((_x1 - _x2) ** 2) / sigma2)\n",
    "\n",
    "def kpca(_X: np.ndarray, k: int, kernel) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    KPCA函数\n",
    "    :param _X: 样本集\n",
    "    :param k: 要保留的非线性主成分数量\n",
    "    :param kernel: 使用的核函数\n",
    "    :return: 投影后的新样本\n",
    "    \"\"\"\n",
    "    sigma2 = 1 / (_X.shape[1] * X.var()) # sigma平方用这个公式进行估计（sklearn中默认也是用这个算法）\n",
    "\n",
    "    # 1. 计算核函数矩阵K\n",
    "    K = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            K[i][j] = kernel(_X[i], _X[j], sigma2)\n",
    "\n",
    "    # 2. 通过特征值分解计算alpha，并且只保留特征值最大的k个非线性主成分\n",
    "    eigen_values, eigen_vectors = np.linalg.eig(K)\n",
    "    eigen_temp = []\n",
    "    for i, eigen_value in enumerate(eigen_values):\n",
    "        eigen_temp.append([eigen_value, i])\n",
    "    eigen_temp.sort()\n",
    "    Alpha = np.zeros((k, n))\n",
    "    for i in range(k):\n",
    "        Alpha[i] = eigen_vectors[eigen_temp[i][1]]\n",
    "\n",
    "    # 3. 计算样本在非线性主成分方向上的投影\n",
    "    X_new = np.zeros((n, k))\n",
    "    for i, x in enumerate(_X):\n",
    "        for l in range(k):\n",
    "            for j in range(n):\n",
    "                X_new[i][l] += Alpha[l][j] * kernel(x, _X[j], sigma2)\n",
    "\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来测试一下保留不同数量非线性主成分时的正确率（为了方便起见我就不分训练集测试集了，直接拿全部数据训练核测试）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "降维之后的样本维度：4  降维之后的正确率为：0.77\n",
      "降维之后的样本维度：20  降维之后的正确率为：0.95\n",
      "降维之后的样本维度：40  降维之后的正确率为：0.99\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = SVC(kernel='rbf')\n",
    "X_new = kpca(X, 4, rbf)\n",
    "svm_classifier.fit(X_new, y)\n",
    "print(\"降维之后的样本维度：\", X_new.shape[1], \"  降维之后的正确率为：\", svm_classifier.score(X_new, y), sep='')\n",
    "X_new = kpca(X, 20, rbf)\n",
    "svm_classifier.fit(X_new, y)\n",
    "print(\"降维之后的样本维度：\", X_new.shape[1], \"  降维之后的正确率为：\", svm_classifier.score(X_new, y), sep='')\n",
    "X_new = kpca(X, 40, rbf)\n",
    "svm_classifier.fit(X_new, y)\n",
    "print(\"降维之后的样本维度：\", X_new.shape[1], \"  降维之后的正确率为：\", svm_classifier.score(X_new, y), sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到KPCA方法的效果在水仙花数据上的表现可以说非常差。\n",
    "\n",
    "原始数据只有4维，但是用KPCA进行特征值分解的得到的特征值数量是和样本数量一样多的，这个数量远远大于4。\n",
    "\n",
    "所以就算“降维”（实际上是升维了）到40个维度，分类正确率才勉强到达99%。\n",
    "\n",
    "这说明KPCA只适用于样本数量少而原始特征维度高的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四、IsoMap方法和LLE方法\n",
    "\n",
    "IsoMap方法把相邻样本看作相连的节点，边长就是欧氏距离，然后算出每两个样本之间的最短路作为距离度量。\n",
    "\n",
    "LLE主要思想是采取分治，把数据切割成一个一个小块，对每个小块求平均抽象成新的样本，然后再进行映射。\n",
    "\n",
    "书上只是简单介绍了一下思想，了解一下就行。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
